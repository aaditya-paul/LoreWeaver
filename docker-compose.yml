version: "3.9"

services:
  # ── FastAPI backend ─────────────────────────────────────────────────────────
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    env_file:
      - ./backend/.env
    environment:
      # Persist DB and vector store inside the mounted volume
      DATABASE_URL: sqlite:////app/persist/loreweaver.db
      CHROMA_DIR: /app/persist/chroma_db
      # Inside Docker, localhost refers to the container itself.
      # Point Ollama to the host machine if you run it there.
      OLLAMA_URL: http://host.docker.internal:11434
    volumes:
      # Single named volume for all persistent data
      - backend_data:/app/persist
    ports:
      - "8000:8000"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/docs"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # ── Flutter web frontend (nginx) ────────────────────────────────────────────
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "8080:80"
    depends_on:
      backend:
        condition: service_healthy
    restart: unless-stopped

  # ── Optional: local Ollama LLM ──────────────────────────────────────────────
  # Uncomment if you want to run a local LLM via Ollama inside Docker.
  # Set LOCAL_LLM_ENABLED=true in backend/.env and pull a model:
  #   docker compose exec ollama ollama pull mistral
  #
  # ollama:
  #   image: ollama/ollama:latest
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_models:/root/.ollama
  #   restart: unless-stopped

volumes:
  backend_data:
  # ollama_models:
